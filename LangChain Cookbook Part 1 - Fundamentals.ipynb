{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "# LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "*This cookbook is based off the [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Goal:** Provide an introductory understanding of the components and use cases of LangChain via [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets. For use cases check out [part 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb). See [video tutorial](https://www.youtube.com/watch?v=2xxziIWmaSA) of this notebook.\n",
    "\n",
    "\n",
    "**Links:**\n",
    "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "**~~TL~~DR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\n",
    "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next\n",
    "\n",
    "### **Why LangChain?**\n",
    "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. **Speed üö¢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Community üë•** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
    "\n",
    "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n",
    "*Note: This cookbook will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "*Update Oct '23: This notebook has been expanded from it's original form*\n",
    "\n",
    "You'll need an OpenAI api key to follow this tutorial. You can have it as an environement variable, in an .env file where this jupyter notebook lives, or insert it below where 'YourAPIKey' is. Have if you have questions on this, put these instructions into [ChatGPT](https://chat.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMMAND_MODE': 'unix2003',\n",
       " 'CONDA_EXE': '/Users/supakornemchananon/anaconda3/anaconda3/bin/conda',\n",
       " 'CONDA_PYTHON_EXE': '/Users/supakornemchananon/anaconda3/anaconda3/bin/python',\n",
       " 'CONDA_SHLVL': '1',\n",
       " 'CPPFLAGS': '-I/opt/homebrew/opt/unixodbc/include',\n",
       " 'DISPLAY': '/private/tmp/com.apple.launchd.1dncoGf3iu/org.xquartz:0',\n",
       " 'DJANGO_ALLOW_ASYNC_UNSAFE': 'true',\n",
       " 'GOOGLE_APPLICATION_CREDENTIALS': '/Users/supakornemchananon/OSEnvironment/google_vision_config.json',\n",
       " 'HOME': '/Users/supakornemchananon',\n",
       " 'HOMEBREW_CELLAR': '/opt/homebrew/Cellar',\n",
       " 'HOMEBREW_OPT': '/opt/homebrew/opt',\n",
       " 'HOMEBREW_PREFIX': '/opt/homebrew',\n",
       " 'HOMEBREW_REPOSITORY': '/opt/homebrew',\n",
       " 'INFOPATH': '/opt/homebrew/share/info:',\n",
       " 'LDFLAGS': '-L/opt/homebrew/opt/unixodbc/lib',\n",
       " 'LOGNAME': 'supakornemchananon',\n",
       " 'MANPATH': '/opt/homebrew/share/man::',\n",
       " 'MallocNanoZone': '0',\n",
       " 'MallocProbGuardViaLaunchd': '1',\n",
       " 'OBJC_DISABLE_INITIALIZE_FORK_SAFETY': 'YES',\n",
       " 'OLDPWD': '/',\n",
       " 'OPENAI_API_KEY': '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',\n",
       " 'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       " 'PATH': '/Users/supakornemchananon/anaconda3/anaconda3/envs/langchaincookbook/bin:/Users/supakornemchananon/anaconda3/anaconda3/condabin:/opt/homebrew/opt/openjdk/bin:/opt/homebrew/opt/libxml2/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools:/usr/local/go/bin:/Users/supakornemchananon/.pub-cache/bin:/opt/homebrew/opt/etc:/opt/homebrew/opt/node@18/bin:/opt/homebrew/opt/mysql-client/lib/pkgconfig:/opt/homebrew/opt/zbar/bin:/Users/supakornemchananon/OSEnvironment:/Users/supakornemchananon/OSEnvironment/flutter/bin:/Users/supakornemchananon/.composer/vendor/bin:/Users/supakornemchananon/rootAVD',\n",
       " 'PKG_CONFIG_PATH': '/opt/homebrew/opt/mysql-client/lib/pkgconfig',\n",
       " 'PWD': '/',\n",
       " 'PYTHONDONTWRITEBYTECODE': '1',\n",
       " 'PYTHONUNBUFFERED': '1',\n",
       " 'SHELL': '/bin/zsh',\n",
       " 'SHLVL': '2',\n",
       " 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.iSGhMxW94c/Listeners',\n",
       " 'TMPDIR': '/var/folders/pc/rt3qdy8n3dbbd2wnyfbt3c_h0000gn/T/',\n",
       " 'USER': 'supakornemchananon',\n",
       " 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       " 'VSCODE_CODE_CACHE_PATH': '/Users/supakornemchananon/Library/Application Support/Code/CachedData/8b617bd08fd9e3fc94d14adb8d358b56e3f72314',\n",
       " 'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       " 'VSCODE_CWD': '/',\n",
       " 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       " 'VSCODE_IPC_HOOK': '/Users/supakornemchananon/Library/Application Support/Code/1.82-main.sock',\n",
       " 'VSCODE_NLS_CONFIG': '{\"locale\":\"en-us\",\"osLocale\":\"en-th\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       " 'VSCODE_PID': '8792',\n",
       " 'XPC_FLAGS': '0x0',\n",
       " 'XPC_SERVICE_NAME': '0',\n",
       " '_': '/Users/supakornemchananon/anaconda3/anaconda3/envs/langchaincookbook/bin/python',\n",
       " '__CFBundleIdentifier': 'com.microsoft.VSCode',\n",
       " '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x0',\n",
       " 'ELECTRON_RUN_AS_NODE': '1',\n",
       " 'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       " 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': '1',\n",
       " 'AZURE_OPENAI_API_VERSION': '2023-07-01-preview',\n",
       " 'CONDA_PROMPT_MODIFIER': '(langchaincookbook) ',\n",
       " 'SERPAPI_API_KEY': '4e14313a66c2052b0e31f445ed254df7727805b8f4d98256afd51c00423b0626',\n",
       " 'PYTHONIOENCODING': 'utf-8',\n",
       " 'AZURE_OPENAI_ENDPOINT': 'https://cogopenaiscgjwdllmchat1.openai.azure.com/',\n",
       " '_CE_CONDA': '',\n",
       " 'CONDA_ROOT': '/Users/supakornemchananon/anaconda3/anaconda3',\n",
       " 'CONDA_PREFIX': '/Users/supakornemchananon/anaconda3/anaconda3/envs/langchaincookbook',\n",
       " '_CE_M': '',\n",
       " 'AZURE_OPENAI_API_KEY': 'dc1475322fbf4c07a1469f57242f14b5',\n",
       " 'CONDA_ALLOW_SOFTLINKS': 'false',\n",
       " 'LC_CTYPE': 'UTF-8',\n",
       " 'CONDA_DEFAULT_ENV': 'langchaincookbook',\n",
       " 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       " 'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'FORCE_COLOR': '1',\n",
       " 'CLICOLOR_FORCE': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       " 'OPENAI_API_VERSION': '2023-07-01-preview'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os\n",
    "\n",
    "config = dict(os.environ)\n",
    "config.update(dotenv_values(\".env\"))\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = config.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = config.get(\"SERPAPI_API_KEY\")\n",
    "os.environ[\"SERP_API_KEY\"] = config.get(\"SERPAPI_API_KEY\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "# LangChain Components\n",
    "\n",
    "## Schema - Nuts and Bolts of working with Large Language Models (LLMs)\n",
    "\n",
    "### **Text**\n",
    "The natural language way to interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {},
   "source": [
    "### **Chat Messages**\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI what to do\n",
    "* **Human** - Messages that are intented to represent the user\n",
    "* **AI** - Messages that show what the AI responded with\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
    "# gpt4-turbo , gpt-35-turbo\n",
    "chat = AzureChatOpenAI(temperature=0, model=\"gpt-35-turbo\", max_tokens=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f7af",
   "metadata": {},
   "source": [
    "Now let's create a few messages that simulate a chat experience with a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a female AI pharmacist advising medication based on patient symptoms\"),\n",
    "    HumanMessage(content=\"‡∏â‡∏±‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß\"),\n",
    "    AIMessage(content=\"‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏¢‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡πÄ‡∏ã‡∏ï‡∏≤‡∏°‡∏≠‡∏• 500 ‡∏°‡∏¥‡∏•‡∏•‡∏¥‡∏Å‡∏£‡∏±‡∏° ‡∏™‡∏≠‡∏á‡πÄ‡∏°‡πá‡∏î ‡∏ó‡∏∏‡∏Å 4-6 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\"),\n",
    "    HumanMessage(content=\"‡∏â‡∏±‡∏ô‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á\"),\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {},
   "source": [
    "You can also pass more chat history w/ responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "    HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "    AIMessage(content=\"You should go to Nice, France\"),\n",
    "    HumanMessage(content=\"What else should I do when I'm there?\"),\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ee37a",
   "metadata": {},
   "source": [
    "You can also exclude the system message if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {},
   "source": [
    "### **Documents**\n",
    "An object that holds a piece of text and metadata (more information about that text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document(\n",
    "    page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "    metadata={\"my_document_id\": 234234, \"my_document_source\": \"The LangChain Papers\", \"my_document_create_time\": 1680013019},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd19754",
   "metadata": {},
   "source": [
    "But you don't have to include metadata if you don't want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {},
   "source": [
    "## Models - The interface to the AI brains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {},
   "source": [
    "### **Chat Model**\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf091777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4260711",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c028f9",
   "metadata": {},
   "source": [
    "### Function Calling Models\n",
    "\n",
    "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
    "\n",
    "This comes in handy when you're making an API call to an external service or doing extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "output = chat(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "        HumanMessage(content=\"What's the weather like in Boston right now?\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f399a1d",
   "metadata": {},
   "source": [
    "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {},
   "source": [
    "### **Text Embedding Model**\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "*BTW: Semantic means 'relating to meaning in language or logic.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c85e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {},
   "source": [
    "## Prompts - Text generally used as instructions to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "What you'll pass to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d270239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.azure_openai import AzureChatOpenAI\n",
    "\n",
    "# llm = AzureChatOpenAI(model_name=\"gpt-35-turbo\", temperature=0)\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(chat([HumanMessage(content=prompt)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts\n",
    "\n",
    "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.azure_openai import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Rome\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [SystemMessage(content=final_prompt)]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {},
   "source": [
    "### **Example Selectors**\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(model=\"text-embedding-ada-002\"),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"dog\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aimessage = chat([HumanMessage(content=similar_prompt.format(noun=my_noun))])\n",
    "aimessage\n",
    "# llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
    "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their [documentation](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "Two big concepts:\n",
    "\n",
    "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "\n",
    "**2. Parser** - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58353756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1079f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"‡∏™‡∏±‡∏ß‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏∏‡∏û ‡∏¢‡∏µ‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏• ‡∏™‡∏π‡πà‡∏Å‡∏£‡∏∏‡∏°‡πÄ‡∏ó‡∏ö‡∏Ø\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chat([HumanMessage(content=promptValue)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985aa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07045ae3",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 2: OpenAI Fuctions**\n",
    "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
    "\n",
    "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
    "\n",
    "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
    "\n",
    "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use `gpt4-0613`\n",
    "\n",
    "**Example 1: Simple**\n",
    "\n",
    "Let's get started by defining a simple model for us to extract from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3593699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "prompt = PromptTemplate(template=\"Use the given format to extract information from the following input: {input}\", input_variables=[\"input\"])\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033d15",
   "metadata": {},
   "source": [
    "Then let's create a chain (more on this later) that will do the extracting for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b7be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Sally', age=13, fav_food=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = create_structured_output_chain(Person, chat, prompt)\n",
    "chain.run(\"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87084e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(common_name='Starbucks', telephone_number=['123-456-7890'], province='Ontario', zipcode='M1B 3C3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpage_text = \"\"\"\"\"\"\n",
    "\n",
    "\n",
    "class Location(BaseModel):\n",
    "    common_name: str = Field(description=\"Location common name\")\n",
    "    telephone_number: Optional[list] = Field(description=\"Phone number of location\", default=None)\n",
    "    province: Optional[str] = Field(description=\"Provice name of location\", default=None)\n",
    "    zipcode: Optional[str] = Field(description=\"Zip code of Location\", default=None)\n",
    "\n",
    "\n",
    "chain = create_structured_output_chain(Location, chat, prompt)\n",
    "chain.run(webpage_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370210",
   "metadata": {},
   "source": [
    "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bc127",
   "metadata": {},
   "source": [
    "Now we'll call for People rather than Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ba430d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class '__main__.People'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/supakornemchananon/Desktop/langchain-cookbook-main/LangChain Cookbook Part 1 - Fundamentals.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/supakornemchananon/Desktop/langchain-cookbook-main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain \u001b[39m=\u001b[39m create_structured_output_chain(People, chat, prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/supakornemchananon/Desktop/langchain-cookbook-main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m chain\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mSally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/langchain/chains/openai_functions/base.py:543\u001b[0m, in \u001b[0;36mcreate_structured_output_chain\u001b[0;34m(output_schema, llm, prompt, output_key, output_parser, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m     function: Any \u001b[39m=\u001b[39m {\n\u001b[1;32m    534\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39moutput_formatter\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m\"\u001b[39m: output_schema,\n\u001b[1;32m    540\u001b[0m     }\n\u001b[1;32m    541\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mclass\u001b[39;00m \u001b[39m_OutputFormatter\u001b[39;00m(BaseModel):\n\u001b[1;32m    544\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    546\u001b[0m         output: output_schema  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         is_untouched(value)\n\u001b[1;32m    191\u001b[0m         \u001b[39mand\u001b[39;00m ann_type \u001b[39m!=\u001b[39m PyObject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     fields[ann_name] \u001b[39m=\u001b[39m ModelField\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m    198\u001b[0m         name\u001b[39m=\u001b[39;49mann_name,\n\u001b[1;32m    199\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    200\u001b[0m         annotation\u001b[39m=\u001b[39;49mann_type,\n\u001b[1;32m    201\u001b[0m         class_validators\u001b[39m=\u001b[39;49mvg\u001b[39m.\u001b[39;49mget_validators(ann_name),\n\u001b[1;32m    202\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39melif\u001b[39;00m ann_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m namespace \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39munderscore_attrs_are_private:\n\u001b[1;32m    205\u001b[0m     private_attributes[ann_name] \u001b[39m=\u001b[39m PrivateAttr()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:506\u001b[0m, in \u001b[0;36mModelField.infer\u001b[0;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[1;32m    503\u001b[0m     required \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m annotation \u001b[39m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[39m.\u001b[39mvalidate_assignment)\n\u001b[0;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    507\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    508\u001b[0m     type_\u001b[39m=\u001b[39;49mannotation,\n\u001b[1;32m    509\u001b[0m     alias\u001b[39m=\u001b[39;49mfield_info\u001b[39m.\u001b[39;49malias,\n\u001b[1;32m    510\u001b[0m     class_validators\u001b[39m=\u001b[39;49mclass_validators,\n\u001b[1;32m    511\u001b[0m     default\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    512\u001b[0m     default_factory\u001b[39m=\u001b[39;49mfield_info\u001b[39m.\u001b[39;49mdefault_factory,\n\u001b[1;32m    513\u001b[0m     required\u001b[39m=\u001b[39;49mrequired,\n\u001b[1;32m    514\u001b[0m     model_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    515\u001b[0m     field_info\u001b[39m=\u001b[39;49mfield_info,\n\u001b[1;32m    516\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:436\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[0;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m SHAPE_SINGLETON\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mprepare_field(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:557\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault \u001b[39mis\u001b[39;00m Undefined \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_factory \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulate_validators()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:831\u001b[0m, in \u001b[0;36mModelField.populate_validators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msub_fields \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m SHAPE_GENERIC:\n\u001b[1;32m    828\u001b[0m     get_validators \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_, \u001b[39m'\u001b[39m\u001b[39m__get_validators__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    829\u001b[0m     v_funcs \u001b[39m=\u001b[39m (\n\u001b[1;32m    830\u001b[0m         \u001b[39m*\u001b[39m[v\u001b[39m.\u001b[39mfunc \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m class_validators_ \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39meach_item \u001b[39mand\u001b[39;00m v\u001b[39m.\u001b[39mpre],\n\u001b[0;32m--> 831\u001b[0m         \u001b[39m*\u001b[39m(get_validators() \u001b[39mif\u001b[39;00m get_validators \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(find_validators(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_config))),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39m[v\u001b[39m.\u001b[39mfunc \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m class_validators_ \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39meach_item \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mpre],\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidators \u001b[39m=\u001b[39m prep_validators(v_funcs)\n\u001b[1;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_validators \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/validators.py:765\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m(type_, config)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[39myield\u001b[39;00m make_arbitrary_type_validator(type_)\n\u001b[1;32m    764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mno validator found for \u001b[39m\u001b[39m{\u001b[39;00mtype_\u001b[39m}\u001b[39;00m\u001b[39m, see `arbitrary_types_allowed` in Config\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class '__main__.People'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "chain = create_structured_output_chain(People, chat, prompt)\n",
    "chain.run(\"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db9b8b",
   "metadata": {},
   "source": [
    "Let's do some more parsing with it\n",
    "\n",
    "**Example 2: Enum**\n",
    "\n",
    "Now let's parse when a product from a list is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5250ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Products(BaseModel):\n",
    "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
    "\n",
    "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_structured_output_chain(Products, chat, prompt)\n",
    "chain.run(\"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {},
   "source": [
    "## Indexes - Structuring documents to LLMs can work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {},
   "source": [
    "### **Document Loaders**\n",
    "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4719d4",
   "metadata": {},
   "source": [
    "**HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee693520",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d89ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564583f",
   "metadata": {},
   "source": [
    "**Books from Gutenberg Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72964fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47140a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].page_content[1855:1984])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1386b0",
   "metadata": {},
   "source": [
    "**URLs and webpages**\n",
    "\n",
    "Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a54e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\"http://www.paulgraham.com/\", \"https://www.scg.com/en/07contact_center/contact.php\"]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {},
   "source": [
    "### **Text Splitters**\n",
    "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95713e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54455f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3090f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e670d",
   "metadata": {},
   "source": [
    "There are a ton of different ways to do text splitting and it really depends on your retrieval strategy and application design. Check out more splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {},
   "source": [
    "### **Retrievers**\n",
    "Easy way to combine documents with language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a66e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0534bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db383cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {},
   "source": [
    "### **VectorStores**\n",
    "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "\n",
    "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5533ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661fdf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ac0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac358c5",
   "metadata": {},
   "source": [
    "Your vectorstore store your embeddings (‚òùÔ∏è) and make them easily searchable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Helping LLMs remember information.\n",
    "\n",
    "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
    "\n",
    "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2949fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {},
   "source": [
    "## Chains ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "Combining different LLM calls and action automatically\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explaining different summarization chain types\n",
    "\n",
    "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
    "\n",
    "We'll cover two of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {},
   "source": [
    "### 1. Simple Sequential Chains\n",
    "\n",
    "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {},
   "source": [
    "### 2. Summarization Chain\n",
    "\n",
    "Easily run through long numerous documents and get a summary. Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for other chain types besides map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f218c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {},
   "source": [
    "## Agents ü§ñü§ñ\n",
    "\n",
    "Official LangChain Documentation describes agents perfectly (emphasis mine):\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a ‚Äúagent‚Äù which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
    "\n",
    "\n",
    "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
    "\n",
    "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "The language model that drives decision making.\n",
    "\n",
    "More specifically, an agent takes in an input and returns a response corresponding to an action to take along with an action input. You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "A 'capability' of an agent. This is an abstraction on top of a function that makes it easy for LLMs (and agents) to interact with it. Ex: Google search.\n",
    "\n",
    "This area shares commonalities with [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "Groups of tools that your agent can select from\n",
    "\n",
    "Let's bring them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46436da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67d5d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import json\n",
    "\n",
    "llm = chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serpapi_api_key = os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n",
    "agent = initialize_agent(toolkit, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4882754",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent({\"input\": \"‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢? ,‡πÇ‡∏î‡∏¢‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b07044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "from serpapi import GoogleSearch\n",
    "from langchain.tools.base import BaseTool, Tool\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "def google_map_api_search(query_message: str):\n",
    "    params = {\n",
    "        \"engine\": \"google_maps\",\n",
    "        \"q\": query_message,\n",
    "        \"type\": \"search\",\n",
    "        \"api_key\": os.environ[\"SERP_API_KEY\"],\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    # local_results = results[\"local_results\"]\n",
    "    return json.dumps(results, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class InputModelGoogleMapApiSearch(BaseModel):\n",
    "    query_message: str = Field(description=\"‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤\")\n",
    "\n",
    "\n",
    "class GoogleMapApiSearchTool(BaseTool):\n",
    "    name = \"google_map_api_search\"\n",
    "    description = \"useful for when you need to search location with google map api.\"\n",
    "    args_schema: Type[InputModelGoogleMapApiSearch] = InputModelGoogleMapApiSearch\n",
    "\n",
    "    def _run(self, query_message: str, *args: Any, **kwargs: Any) -> Any:\n",
    "        return google_map_api_search(query_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb36599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"search_metadata\": {\"id\": \"658ba9d31baebb7b67245d13\", \"status\": \"Success\", \"json_endpoint\": \"https://serpapi.com/searches/57edc1b11ce0e023/658ba9d31baebb7b67245d13.json\", \"created_at\": \"2023-12-27 04:36:35 UTC\", \"processed_at\": \"2023-12-27 04:36:35 UTC\", \"google_maps_url\": \"https://www.google.com/maps/search/%E0%B9%80%E0%B8%A1%E0%B8%8B%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%8C%E0%B9%80%E0%B8%94%E0%B9%89%E0%B8%99%E0%B8%97%E0%B9%8C%E0%B8%9D%E0%B8%B2%E0%B8%81%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%AD%E0%B8%99%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%A3%E0%B8%B8%E0%B8%87%E0%B9%80%E0%B8%97%E0%B8%9E%E0%B8%A1%E0%B8%AB%E0%B8%B2%E0%B8%99%E0%B8%84%E0%B8%A3?hl=en\", \"raw_html_file\": \"https://serpapi.com/searches/57edc1b11ce0e023/658ba9d31baebb7b67245d13.html\", \"total_time_taken\": 5.2}, \"search_parameters\": {\"engine\": \"google_maps\", \"type\": \"search\", \"q\": \"‡πÄ‡∏°‡∏ã‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πå‡πÄ‡∏î‡πâ‡∏ô‡∏ó‡πå‡∏ù‡∏≤‡∏Å‡∏°‡∏≤‡∏î‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\", \"google_domain\": \"google.com\", \"hl\": \"en\"}, \"search_information\": {\"local_results_state\": \"Results for exact spelling\", \"query_displayed\": \"‡πÄ‡∏°‡∏ã‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πå‡πÄ‡∏î‡πâ‡∏ô‡∏ó‡πå‡∏ù‡∏≤‡∏Å‡∏°‡∏≤‡∏î‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\"}, \"local_results\": [{\"position\": 1, \"title\": \"Maison Garden Condo 2\", \"place_id\": \"ChIJ3Yn21F6D4jAR0iXthU2Emo4\", \"data_id\": \"0x30e2835ed4f689dd:0x8e9a844d85ed25d2\", \"data_cid\": \"10275670968293467602\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x30e2835ed4f689dd%3A0x8e9a844d85ed25d2&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x30e2835ed4f689dd%3A0x8e9a844d85ed25d2&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.8961767, \"longitude\": 100.57072289999999}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJ3Yn21F6D4jAR0iXthU2Emo4\", \"provider_id\": \"/g/11fjdwv__9\", \"rating\": 4.5, \"reviews\": 16, \"type\": \"‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏°‡∏¥‡πÄ‡∏ô‡∏µ‡∏¢‡∏°\", \"types\": [\"‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏°‡∏¥‡πÄ‡∏ô‡∏µ‡∏¢‡∏°\"], \"address\": \"‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà 98 Soi Chaeng Watthana 12 Yaek 4-7, Thung Song Hong, Lak Si, Bangkok 10210, Thailand\", \"open_state\": \"Open ‚ãÖ Closes 6\\u202fPM\", \"hours\": \"Open ‚ãÖ Closes 6\\u202fPM\", \"operating_hours\": {\"wednesday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"thursday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"friday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"saturday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"sunday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"monday\": \"9\\u202fAM‚Äì6\\u202fPM\", \"tuesday\": \"9\\u202fAM‚Äì6\\u202fPM\"}, \"phone\": \"+66 81 362 4444\", \"website\": \"http://www.maisongarden.com/\", \"thumbnail\": \"https://lh5.googleusercontent.com/p/AF1QipNlKibo7LpQLuh_n0bl8IuBfhWjWTQ-anAhsQ-n=w153-h92-k-no\"}, {\"position\": 2, \"title\": \"Maison Garden Condo\", \"place_id\": \"ChIJY7CJguGC4jARgGLF5j-1CjY\", \"data_id\": \"0x30e282e18289b063:0x360ab53fe6c56280\", \"data_cid\": \"3894124113874477696\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x30e282e18289b063%3A0x360ab53fe6c56280&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x30e282e18289b063%3A0x360ab53fe6c56280&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.8980617, \"longitude\": 100.57736129999999}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJY7CJguGC4jARgGLF5j-1CjY\", \"provider_id\": \"/g/11dx8pd2_c\", \"rating\": 4.5, \"reviews\": 15, \"type\": \"‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏°‡∏¥‡πÄ‡∏ô‡∏µ‡∏¢‡∏°\", \"types\": [\"‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏°‡∏¥‡πÄ‡∏ô‡∏µ‡∏¢‡∏°\"], \"address\": \"248 Chaeng Watthana 10 Alley, Lane 9-1-8, Thung Song Hong, Lak Si, Bangkok 10210, Thailand\", \"phone\": \"+66 85 103 8888\", \"website\": \"http://www.maisongarden.com/maison-garden-1/\", \"thumbnail\": \"https://lh5.googleusercontent.com/p/AF1QipMzzLv9kz56ekCDzMWEkqzM2ZbHFdlg1qA8InOd=w100-h92-k-no\"}], \"serpapi_pagination\": {\"next\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&q=%E0%B9%80%E0%B8%A1%E0%B8%8B%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%8C%E0%B9%80%E0%B8%94%E0%B9%89%E0%B8%99%E0%B8%97%E0%B9%8C%E0%B8%9D%E0%B8%B2%E0%B8%81%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%AD%E0%B8%99%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%A3%E0%B8%B8%E0%B8%87%E0%B9%80%E0%B8%97%E0%B8%9E%E0%B8%A1%E0%B8%AB%E0%B8%B2%E0%B8%99%E0%B8%84%E0%B8%A3&start=20&type=search\"}}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_message = \"‡πÄ‡∏°‡∏ã‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πå‡πÄ‡∏î‡πâ‡∏ô‡∏ó‡πå‡∏ù‡∏≤‡∏Å‡∏°‡∏≤‡∏î‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\"\n",
    "map_tool = GoogleMapApiSearchTool()\n",
    "response = map_tool.run(tool_input={\"query_message\": query_message})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5b2b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent([map_tool], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, return_intermediate_steps=True, max_iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06b847d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the google_map_api_search tool to find the location mentioned in the query message.\n",
      "Action: google_map_api_search\n",
      "Action Input: ‡∏ò‡∏¥‡πÇ‡∏≠‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå-‡πÅ‡∏à‡πâ‡∏á‡∏Ø ‡∏õ.20 EILEEN ‡∏à‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{\"search_metadata\": {\"id\": \"658bb53ac90e0afa5f2d56f7\", \"status\": \"Success\", \"json_endpoint\": \"https://serpapi.com/searches/1bb48ce7a6e20fde/658bb53ac90e0afa5f2d56f7.json\", \"created_at\": \"2023-12-27 05:25:14 UTC\", \"processed_at\": \"2023-12-27 05:25:14 UTC\", \"google_maps_url\": \"https://www.google.com/maps/search/%E0%B8%98%E0%B8%B4%E0%B9%82%E0%B8%AD%E0%B8%8A%E0%B8%B1%E0%B8%A2%E0%B8%9E%E0%B8%A4%E0%B8%81%E0%B8%A9%E0%B9%8C-%E0%B9%81%E0%B8%88%E0%B9%89%E0%B8%87%E0%B8%AF+%E0%B8%9B.20+EILEEN+%E0%B8%88%E0%B8%99%E0%B8%99%E0%B8%97%E0%B8%9A%E0%B8%B8%E0%B8%A3%E0%B8%B5?hl=en\", \"raw_html_file\": \"https://serpapi.com/searches/1bb48ce7a6e20fde/658bb53ac90e0afa5f2d56f7.html\", \"total_time_taken\": 0.55}, \"search_parameters\": {\"engine\": \"google_maps\", \"type\": \"search\", \"q\": \"‡∏ò‡∏¥‡πÇ‡∏≠‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå-‡πÅ‡∏à‡πâ‡∏á‡∏Ø ‡∏õ.20 EILEEN ‡∏à‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\", \"google_domain\": \"google.com\", \"hl\": \"en\"}, \"search_information\": {\"local_results_state\": \"Results for exact spelling\", \"query_displayed\": \"‡∏ò‡∏¥‡πÇ‡∏≠‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå-‡πÅ‡∏à‡πâ‡∏á‡∏Ø ‡∏õ.20 EILEEN ‡∏à‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\"}, \"local_results\": [{\"position\": 1, \"title\": \"‡∏ñ. ‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå\", \"place_id\": \"ChIJa8XN-wCF4jARTOAyMQ_od0Y\", \"data_id\": \"0x30e28500fbcdc56b:0x4677e80f3132e04c\", \"data_cid\": \"5077782256831094860\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x30e28500fbcdc56b%3A0x4677e80f3132e04c&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x30e28500fbcdc56b%3A0x4677e80f3132e04c&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.925834499999999, \"longitude\": 100.46304479999999}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJa8XN-wCF4jARTOAyMQ_od0Y\", \"provider_id\": \"/g/1215_hvx\", \"address\": \"Chang Wat Nonthaburi, Thailand\", \"thumbnail\": \"https://lh5.googleusercontent.com/p/AF1QipN0a7ObcOYhGgBehs3HfqT-49Y7IU3nwoofzW9v=w360-h203-k-no\"}], \"serpapi_pagination\": {\"next\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&q=%E0%B8%98%E0%B8%B4%E0%B9%82%E0%B8%AD%E0%B8%8A%E0%B8%B1%E0%B8%A2%E0%B8%9E%E0%B8%A4%E0%B8%81%E0%B8%A9%E0%B9%8C-%E0%B9%81%E0%B8%88%E0%B9%89%E0%B8%87%E0%B8%AF+%E0%B8%9B.20+EILEEN+%E0%B8%88%E0%B8%99%E0%B8%99%E0%B8%97%E0%B8%9A%E0%B8%B8%E0%B8%A3%E0%B8%B5&start=20&type=search\"}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe location closest to the query message is \"‡∏ñ. ‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå\" in Chang Wat Nonthaburi, Thailand.\n",
      "Final Answer: ‡∏ñ. ‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå (Chang Wat Nonthaburi)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ location ‡∏ô‡∏µ‡πâ\n",
    "‡πÇ‡∏î‡∏¢‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠ Location ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 1 ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ\n",
    "format output = common name + province (optional)\n",
    "\n",
    "<query_message>\n",
    "{query_message}\n",
    "</query_message>\n",
    "\"\"\",\n",
    "    input_variables=[\"query_message\"],\n",
    ")\n",
    "\n",
    "query_message = \"‡∏ò‡∏¥‡πÇ‡∏≠‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå-‡πÅ‡∏à‡πâ‡∏á‡∏Ø ‡∏õ.20 EILEEN ‡∏à‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\"\n",
    "response = agent({\"input\": prompt_template.format(query_message=query_message)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00c84f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏ñ. ‡∏ä‡∏±‡∏¢‡∏û‡∏§‡∏Å‡∏©‡πå (Chang Wat Nonthaburi)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc48110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class SearchLocation(BaseModel):\n",
    "    common_name: str = Field(description=\"Location common name\")\n",
    "    address: str = Field(description=\"Address of location\")\n",
    "    lat_lng: str = Field(description=\"Latutude and Longtitude of Location\")\n",
    "    telephone_number: list = Field(description=\"Phone number of location\", default=None)\n",
    "    province: str = Field(description=\"Provice name of location\", default=None)\n",
    "    zipcode: str = Field(description=\"Zip code of Location\", default=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cabd482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"search_metadata\": {\"id\": \"658ba9825c5606b7435fadc2\", \"status\": \"Success\", \"json_endpoint\": \"https://serpapi.com/searches/bcc2b65f997966e7/658ba9825c5606b7435fadc2.json\", \"created_at\": \"2023-12-27 04:35:14 UTC\", \"processed_at\": \"2023-12-27 04:35:14 UTC\", \"google_maps_url\": \"https://www.google.com/maps/search/%E0%B8%A1.%E0%B8%81%E0%B8%B5%E0%B8%A3%E0%B8%95%E0%B8%B4%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B8%AA%E0%B8%A7%E0%B8%99%E0%B8%8B.11%23%E0%B8%8A%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%A5%E0%B9%87%E0%B8%81%E0%B8%88%E0%B8%8A%E0%B8%A5%E0%B8%9A%E0%B8%B8%E0%B8%A3%E0%B8%B5?hl=en\", \"raw_html_file\": \"https://serpapi.com/searches/bcc2b65f997966e7/658ba9825c5606b7435fadc2.html\", \"total_time_taken\": 0.73}, \"search_parameters\": {\"engine\": \"google_maps\", \"type\": \"search\", \"q\": \"‡∏°.‡∏Å‡∏µ‡∏£‡∏ï‡∏¥‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏ô‡∏ã.11#‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ\", \"google_domain\": \"google.com\", \"hl\": \"en\"}, \"search_information\": {\"local_results_state\": \"Results for exact spelling\", \"query_displayed\": \"‡∏°.‡∏Å‡∏µ‡∏£‡∏ï‡∏¥‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏ô‡∏ã.11#‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ\"}, \"local_results\": [{\"position\": 1, \"title\": \"‡∏Å‡∏µ‡∏£‡∏ï‡∏¥\", \"place_id\": \"ChIJNadBa5s1HTERRNz9bDI9ktk\", \"data_id\": \"0x311d359b6b41a735:0xd9923d326cfddc44\", \"data_cid\": \"15677660539616353348\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x311d359b6b41a735%3A0xd9923d326cfddc44&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x311d359b6b41a735%3A0xd9923d326cfddc44&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.3551564, \"longitude\": 100.994872}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJNadBa5s1HTERRNz9bDI9ktk\", \"provider_id\": \"/g/11fx8zrl7k\", \"address\": \"Tambon Ban Suan, Amphoe Mueang Chon Buri, Chang Wat Chon Buri 20000, Thailand\"}, {\"position\": 2, \"title\": \"‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏ô ‡∏ã‡∏≠‡∏¢11\", \"place_id\": \"ChIJU9FFL3A2HTER9GmydFkFrB0\", \"data_id\": \"0x311d36702f45d153:0x1dac055974b269f4\", \"data_cid\": \"2138089804862220788\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x311d36702f45d153%3A0x1dac055974b269f4&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x311d36702f45d153%3A0x1dac055974b269f4&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.4054896, \"longitude\": 101.0120962}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJU9FFL3A2HTER9GmydFkFrB0\", \"provider_id\": \"/g/11dxdl39jr\", \"address\": \"Tambon Nong Mai Daeng, Amphoe Mueang Chon Buri, Chang Wat Chon Buri 20000, Thailand\", \"thumbnail\": \"https://lh5.googleusercontent.com/p/AF1QipONwEzvrVmsAsJHgX0ptAaQebibxvBUEuD-R1Sy=w80-h106-k-no\"}, {\"position\": 3, \"title\": \"‡∏ã‡∏≠‡∏¢ 11\", \"place_id\": \"ChIJS2UabqI1HTERy5vwLEuGxjQ\", \"data_id\": \"0x311d35a26e1a654b:0x34c6864b2cf09bcb\", \"data_cid\": \"3802874592795597771\", \"reviews_link\": \"https://serpapi.com/search.json?data_id=0x311d35a26e1a654b%3A0x34c6864b2cf09bcb&engine=google_maps_reviews&hl=en\", \"photos_link\": \"https://serpapi.com/search.json?data_id=0x311d35a26e1a654b%3A0x34c6864b2cf09bcb&engine=google_maps_photos&hl=en\", \"gps_coordinates\": {\"latitude\": 13.3563949, \"longitude\": 101.00429109999999}, \"place_id_search\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&place_id=ChIJS2UabqI1HTERy5vwLEuGxjQ\", \"provider_id\": \"/g/11d_77djzk\", \"address\": \"Tambon Ban Suan, Amphoe Mueang Chon Buri, Chang Wat Chon Buri 20000, Thailand\"}], \"serpapi_pagination\": {\"next\": \"https://serpapi.com/search.json?engine=google_maps&google_domain=google.com&hl=en&q=%E0%B8%A1.%E0%B8%81%E0%B8%B5%E0%B8%A3%E0%B8%95%E0%B8%B4%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B8%AA%E0%B8%A7%E0%B8%99%E0%B8%8B.11%23%E0%B8%8A%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%A5%E0%B9%87%E0%B8%81%E0%B8%88%E0%B8%8A%E0%B8%A5%E0%B8%9A%E0%B8%B8%E0%B8%A3%E0%B8%B5&start=20&type=search\"}}\n"
     ]
    }
   ],
   "source": [
    "query_message = \"‡∏°.‡∏Å‡∏µ‡∏£‡∏ï‡∏¥‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏ô‡∏ã.11#‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏à‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ\"\n",
    "response = map_tool.run(tool_input={\"query_message\": query_message})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8605b91a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class '__main__.SearchLocation'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/supakornemchananon/Desktop/langchain-cookbook-main/LangChain Cookbook Part 1 - Fundamentals.ipynb Cell 131\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/supakornemchananon/Desktop/langchain-cookbook-main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb#Y304sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain \u001b[39m=\u001b[39m create_structured_output_chain(SearchLocation, chat, prompt)\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/langchain/chains/openai_functions/base.py:543\u001b[0m, in \u001b[0;36mcreate_structured_output_chain\u001b[0;34m(output_schema, llm, prompt, output_key, output_parser, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m     function: Any \u001b[39m=\u001b[39m {\n\u001b[1;32m    534\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39moutput_formatter\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m\"\u001b[39m: output_schema,\n\u001b[1;32m    540\u001b[0m     }\n\u001b[1;32m    541\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mclass\u001b[39;00m \u001b[39m_OutputFormatter\u001b[39;00m(BaseModel):\n\u001b[1;32m    544\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    546\u001b[0m         output: output_schema  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         is_untouched(value)\n\u001b[1;32m    191\u001b[0m         \u001b[39mand\u001b[39;00m ann_type \u001b[39m!=\u001b[39m PyObject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     fields[ann_name] \u001b[39m=\u001b[39m ModelField\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m    198\u001b[0m         name\u001b[39m=\u001b[39;49mann_name,\n\u001b[1;32m    199\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    200\u001b[0m         annotation\u001b[39m=\u001b[39;49mann_type,\n\u001b[1;32m    201\u001b[0m         class_validators\u001b[39m=\u001b[39;49mvg\u001b[39m.\u001b[39;49mget_validators(ann_name),\n\u001b[1;32m    202\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39melif\u001b[39;00m ann_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m namespace \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39munderscore_attrs_are_private:\n\u001b[1;32m    205\u001b[0m     private_attributes[ann_name] \u001b[39m=\u001b[39m PrivateAttr()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:506\u001b[0m, in \u001b[0;36mModelField.infer\u001b[0;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[1;32m    503\u001b[0m     required \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m annotation \u001b[39m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[39m.\u001b[39mvalidate_assignment)\n\u001b[0;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    507\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    508\u001b[0m     type_\u001b[39m=\u001b[39;49mannotation,\n\u001b[1;32m    509\u001b[0m     alias\u001b[39m=\u001b[39;49mfield_info\u001b[39m.\u001b[39;49malias,\n\u001b[1;32m    510\u001b[0m     class_validators\u001b[39m=\u001b[39;49mclass_validators,\n\u001b[1;32m    511\u001b[0m     default\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    512\u001b[0m     default_factory\u001b[39m=\u001b[39;49mfield_info\u001b[39m.\u001b[39;49mdefault_factory,\n\u001b[1;32m    513\u001b[0m     required\u001b[39m=\u001b[39;49mrequired,\n\u001b[1;32m    514\u001b[0m     model_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    515\u001b[0m     field_info\u001b[39m=\u001b[39;49mfield_info,\n\u001b[1;32m    516\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:436\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[0;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m SHAPE_SINGLETON\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mprepare_field(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:557\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault \u001b[39mis\u001b[39;00m Undefined \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_factory \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulate_validators()\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/fields.py:831\u001b[0m, in \u001b[0;36mModelField.populate_validators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msub_fields \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m SHAPE_GENERIC:\n\u001b[1;32m    828\u001b[0m     get_validators \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_, \u001b[39m'\u001b[39m\u001b[39m__get_validators__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    829\u001b[0m     v_funcs \u001b[39m=\u001b[39m (\n\u001b[1;32m    830\u001b[0m         \u001b[39m*\u001b[39m[v\u001b[39m.\u001b[39mfunc \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m class_validators_ \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39meach_item \u001b[39mand\u001b[39;00m v\u001b[39m.\u001b[39mpre],\n\u001b[0;32m--> 831\u001b[0m         \u001b[39m*\u001b[39m(get_validators() \u001b[39mif\u001b[39;00m get_validators \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(find_validators(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_config))),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39m[v\u001b[39m.\u001b[39mfunc \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m class_validators_ \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39meach_item \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mpre],\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidators \u001b[39m=\u001b[39m prep_validators(v_funcs)\n\u001b[1;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_validators \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/anaconda3/envs/langchaincookbook/lib/python3.10/site-packages/pydantic/v1/validators.py:765\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m(type_, config)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[39myield\u001b[39;00m make_arbitrary_type_validator(type_)\n\u001b[1;32m    764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mno validator found for \u001b[39m\u001b[39m{\u001b[39;00mtype_\u001b[39m}\u001b[39;00m\u001b[39m, see `arbitrary_types_allowed` in Config\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class '__main__.SearchLocation'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "chain = create_structured_output_chain(SearchLocation, chat, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
